{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 关于 iPython Notebooks 和 本次实验\n",
    "- Python、Jupyter Notebook、Anaconda\n",
    "    - Python：计算机编程语言；解释器\n",
    "    - Jupyter Notebook：一种iPython Notebooks，适用于Python编程\n",
    "    - Anaconda：一个囊括了Python、Jupyter Notebook等软件的平台；包管理，环境管理\n",
    "\n",
    "- iPython Notebooks：交互式编码环境，在网页端进行展示和运行（VSCode通过插件进行兼容）\n",
    "    - 基本单元（代码块）：Cell\n",
    "    - 基本功能1：编写Python代码（在代码属性的 Cell 中）\n",
    "    - 基本功能2：编写说明文档（在 Markdown 属性的 Cell 中）\n",
    "    - 编写方式：双击 Cell 进入编写模式\n",
    "    - 执行方式：对于选中的 Cell，通过键盘键入 \"SHIFT\"+\"ENTER\" 或者鼠标点击 \"Run Cell\" 将执行其中的代码\n",
    "\n",
    "- Notebook vs. 脚本\n",
    "    - 后缀\n",
    "    - Notebook 可以通过 Markdown 语法撰写带格式的说明文档，支持HTML、Latex；脚本中一般通过注释进行说明，且不具有格式功能\n",
    "    - Notebook 的代码块以 Cell 为单元，Cell之间没有必然的顺序关系，用户可指定运行哪个Cell，运行完系统会对该Cell进行编号\n",
    "    - Notebook 适合进行交互式展示；脚本适合于自动化的应用程序\n",
    "\n",
    "- 准备工作\n",
    "    - 安装Anaconda（意味着Python、pip、Jupyter Notebook等软件全部安装好）\n",
    "    - 创建虚拟环境，在其中安装并配置Jupyter内核\n",
    "    - 第三方包安装及管理（建议先设置源）\n",
    "    - 安装PyCharm（或者VS Code及其插件）\n",
    "    - 在PyCharm配置NB（1.**cd到工作目录**；2.进入base并启动Jupyter；3.在PyCharm中配置带token的URL并选择内核）\n",
    "- 自行探索\n",
    "    - Git代码仓库\n",
    "    - 云平台（例如百度AIStudio）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 练习 1 - \"Hello World\"\n",
    "* **目标**：将字符串 `\"Hello World\"` 赋值给变量 `test`\n",
    "* **方式**：在下方 Cell 的指定位置补充Python代码，实现上述目标\n",
    "* **自测方法**：运行测试代码，对比 “*测试结果*” 和 “*Expected output*”，或者查看 *提示*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ecb05267db45908a4d1c3f727eeb1c",
     "grade": false,
     "grade_id": "cell-edef848c738402d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (≈ 1 line of code)\n",
    "# YOUR CODE STARTS HERE\n",
    "test =   # 请在此处作答，下同，将不再提示\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "test: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 使用 Numpy 实现一些基本功能\n",
    "## 2.1 Numpy简介\n",
    "- 是一种可以被Python调用的科学计算包([www.numpy.org](www.numpy.org))\n",
    "- 核心功能函数包括： `np.exp`， `np.log`， `np.reshape` 等\n",
    "- 安装：`pip install numpy`\n",
    "\n",
    "## 2.2 Sigmoid函数和 np.exp()\n",
    "### 2.2.1 Sigmoid函数\n",
    "Sigmoid函数（见公式1和下图）也被称为logistic函数，他是一个非线性的函数，常用于机器学习（逻辑回归）和深度学习\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}\\tag{1}$$ \n",
    "\n",
    "<img src=\"images/Sigmoid.png\" width=1000>\n",
    "\n",
    "### 练习2 - basic_sigmoid\n",
    "- **目标**：编写一个函数，返回给定实数 `x` 的sigmoid值\n",
    "- **要求**：使用 `math.exp()` 来实现Sigmoid函数中的幂操作（后面将使用 `numpy.exp()` 来替代）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cec1a2c77dcc9c6a59470e3daf70f45",
     "grade": false,
     "grade_id": "cell-7f38ddeceef22374",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from lib_public_tests import *\n",
    "\"\"\"\n",
    "Function:\n",
    "    Compute sigmoid of x.\n",
    "Arguments:\n",
    "    x -- A scalar\n",
    "Return:\n",
    "    s -- sigmoid(x)\n",
    "\"\"\"\n",
    "def basic_sigmoid(x):\n",
    "    # (≈ 1 line of code)\n",
    "    # s =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "150772e06208b50e91305b4ecf1421d4",
     "grade": true,
     "grade_id": "cell-6a7680d0a31b818e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))\n",
    "basic_sigmoid_test(basic_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，`math`库很少在DL中被使用，因为在DL中处理的数据往往是矩阵或者向量，而非实数。下面将basic_sigmoid函数的输入数据改为向量，此时运行将会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]  # x becomes a python list object\n",
    "# basic_sigmoid(x) # 取消注释并运行，你将会看到错误提示，原因是此时 x 是一个向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 numpy的基本使用方法\n",
    "- 导入 numpy 包：python中采用关键字 `import` 来导入包\n",
    "- `numpy.array()`：numpy 中表示一个实数、一个向量、或者一个矩阵的方法 \n",
    "\n",
    "下面将展示采用 numpy 来进行向量幂运算的方法（更多numpy的使用方法可查看[官方文档](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html)，或者运行例如 `np.exp?` 、`help(np.exp)`来获取与该函数相关的参考信息）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# example of np.exp\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(np.exp(t_x))  # result is (exp(1), exp(2), exp(3))\n",
    "t_x += 3  # Python中的广播\n",
    "print(t_x)\n",
    "help(np.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 3 - sigmoid\n",
    "- **目标**：使用 numpy 来实现改进版本的 sigmoid 函数\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcab43fa930612b05f46fdd1421443db",
     "grade": false,
     "grade_id": "cell-4c5ca880d9cf9642",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Compute the sigmoid of x\n",
    "Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "Return:\n",
    "    s -- sigmoid(x)\n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    # (≈ 1 line of code)\n",
    "    # s =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d6c9b80614b72a798e6df324bf20051",
     "grade": true,
     "grade_id": "cell-215cfe583f712716",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Sigmoid 的梯度\n",
    "在DL的反向传播中，需要进行损失函数的梯度计算。让我们先来实现计算 Sigmoid 函数梯度的函数。\n",
    "\n",
    "### 练习 4 - sigmoid_derivative\n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x)) \\tag{3}$$\n",
    "\n",
    "其中，$ \\sigma(x) = \\frac{1}{1+e^{-x}}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "345fa4e729d4a65fc75c7b67b571b053",
     "grade": false,
     "grade_id": "cell-3e66ce00e171b40b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "Return:\n",
    "    ds -- Your computed gradient.\n",
    "\"\"\"\n",
    "def sigmoid_derivative(x):\n",
    "    # (≈ 2 lines of code)\n",
    "    # s =\n",
    "    # ds =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db2f9ff9a137194ea17617bb758e1897",
     "grade": true,
     "grade_id": "cell-1b027673871951a1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))\n",
    "sigmoid_derivative_test(sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Reshaping\n",
    "- 一张数字图片可表示为一个3维数组，其尺寸/形状（shape）为 $(length, height, depth = 3)$. \n",
    "- 在输入给一个DL模型时，图片将被转换为$(length*height*3, 1)$的尺寸. \n",
    "- 上述从3维数组到1维向量的转换过程通常被称为 \"unroll\" 或者 \"reshape\"\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "### 练习 5 - image2vector\n",
    "- **目标**：实现一个名为 `image2vector()` 的函数，接受形状为 (length, height, 3) 的数据，返回形为 (length\\*height\\*3, 1)的向量. \n",
    "- **提示**：如果要将形状为(a, b, c)的数组 v 转换为 (a*b,c) 的形状，可以通过如下方式：\n",
    "``` python\n",
    "v = v.reshape(v.shape[0] * v.shape[1], v.shape[2]) \n",
    "```\n",
    "或者\n",
    "``` python\n",
    "v = v.reshape(-1, v.shape[2]) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdcbf18137f7cfa2d6ca62c4cf5c9c5d",
     "grade": false,
     "grade_id": "cell-b68b7900fdd239cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "\"\"\"\n",
    "def image2vector(image):\n",
    "    # (≈ 1 line of code)\n",
    "    # v =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "339250a81053c3773a053c91cc46fb41",
     "grade": true,
     "grade_id": "cell-3b78eb8b041424f7",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[0.67826139,  0.29380381],\n",
    "                     [0.90714982,  0.52835647],\n",
    "                     [0.4215251,  0.45017551]],\n",
    "                   [[0.92814219,  0.96677647],\n",
    "                    [0.85304703,  0.52351845],\n",
    "                    [0.19981397,  0.27417313]],\n",
    "                   [[0.60659855,  0.00533165],\n",
    "                    [0.10820313,  0.49978937],\n",
    "                    [0.34144279,  0.94630077]]])\n",
    "print(\"image2vector(image) = \" + str(image2vector(t_image)))\n",
    "image2vector_test(image2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 归一化行\n",
    "- **定义**：将 x 更改为 $ \\frac{x}{\\| x\\|} $ (将 x 的每一行除以他的模).\n",
    "- **动机**：对数据进行归一化常常能够获取更好的系统性能，因为归一化有利于梯度下降的收敛速度. \n",
    "- **例子**：\n",
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{4}$$ \n",
    "则 \n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{5} $$\n",
    "那么\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{6}$$ \n",
    "- `numpy.linalg.norm()`函数的参数说明：\n",
    "    - `keepdims=True` 确保不会改变结果的维度（即输出保持与输入一致的维度，方便后续操作。很多其他`numpy`函数都有该参数）\n",
    "    - `axis=1` 意味着按行实时归一化，而非若 `axis=0` 则是按照列进行归一化.  \n",
    "\n",
    "### 练习 6 - normalize_rows\n",
    "- **目标**：实现函数 normalizeRows() 用于对给定矩阵进行行归一化. \n",
    "- **注意**：不要使用 `x /= x_norm`，因为广播操作没有对 `/=` 操作符进行重载."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc112a436b66b6526a5fbfe1e7822ba8",
     "grade": false,
     "grade_id": "cell-5a030834cece94f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "\"\"\"\n",
    "def normalize_rows(x):\n",
    "    # (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # x_norm =\n",
    "    # Divide x by its norm.\n",
    "    # x =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c780d41666f3144b0d68804c2df2e21",
     "grade": true,
     "grade_id": "cell-0910101c4de92095",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))\n",
    "normalizeRows_test(normalize_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.linalg.norm` 还有另外一个名为 `ord` 的参数，其取值不同时，对应着不同的归一化方法（例如后面介绍的L1和L2范数）。访问链接 [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) 查看详细的说明文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 7 - softmax\n",
    "- **目标**：实现一个softmax函数, 在DL中用于分类. \n",
    "- **提示**：可以将softmax看做一种归一化函数，其定义如下：\n",
    "1. 对于$ x \\in \\mathbb{R}^{1\\times m}$，\n",
    "\n",
    "$$softmax(x)=softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_m  \n",
    "\\end{bmatrix})\\\\\n",
    "\\begin{bmatrix}\n",
    "\\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_m}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \\tag{7}\n",
    "$$ \n",
    "\n",
    "2. 对于矩阵  $x \\in \\mathbb{R}^{n \\times m}$,  $x_{ij}$ 表示 $x$ 中第 $i^{th}$ 行第 $j^{th}$ 列元素，则\n",
    "\n",
    "$$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1m} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{nm}\n",
    "    \\end{bmatrix} \\\\ \n",
    "    =\\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1m}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2m}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{n1}}}{\\sum_{j}e^{x_{nj}}} & \\frac{e^{x_{n2}}}{\\sum_{j}e^{x_{nj}}} & \\frac{e^{x_{n3}}}{\\sum_{j}e^{x_{nj}}} & \\dots  & \\frac{e^{x_{nm}}}{\\sum_{j}e^{x_{nj}}}\n",
    "    \\end{bmatrix} \\\\\n",
    "    = \\begin{bmatrix}\n",
    "        softmax\\text{(x的第一行)}  \\\\\n",
    "        softmax\\text{(x的第二行)} \\\\\n",
    "        \\vdots  \\\\\n",
    "        softmax\\text{(x的最后一行)} \\\\\n",
    "    \\end{bmatrix} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5100054e6e6ea2c9a6343b43406cf909",
     "grade": false,
     "grade_id": "cell-f41746c0a00bd2fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Calculates the softmax for each row of the input x.\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    # (≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    # x_exp = ...\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # x_sum = ...\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # s = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e598fb22ddfec51bbdb6d08af1076cc5",
     "grade": true,
     "grade_id": "cell-6f8e1f025948128c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0, 0]])\n",
    "print(\"softmax(x) = \" + str(softmax(t_x)))\n",
    "softmax_test(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "<b>What you need to remember:</b>\n",
    "    \n",
    "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "- the sigmoid function and its gradient\n",
    "- image2vector is commonly used in deep learning\n",
    "- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n",
    "- numpy has efficient built-in functions\n",
    "- broadcasting is extremely useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 向量化\n",
    "向量化可以确保在处理大规模数据时，提升算法的效率。\n",
    "\n",
    "## 3.1 内（dot）积、外（outer）积、逐元素（elementwise）乘积\n",
    "- 对于等长向量 $p=[p_1, p_2, \\cdots, p_n]$ 和 $q=[q_1, q_2, \\cdots, q_n]$ 来说，其 dot product， outer product 和 elementwise product 的定义分别为公式（9）、（10）和（11）\n",
    "$$\\begin{bmatrix}p_1, p_2, \\cdots, p_n\\end{bmatrix} \\begin{bmatrix}\n",
    "    q_1 \\\\\n",
    "    q_2 \\\\\n",
    "    \\vdots \\\\ \n",
    "    q_n\n",
    "    \\end{bmatrix} = p_1q_1+p_2q_2+ \\cdots +p_nq_n \n",
    "    = \n",
    "\\sum_{i=1}^n{p_iq_i}\\tag{9}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    p_1 \\\\\n",
    "    p_2 \\\\\n",
    "    \\vdots \\\\ \n",
    "    p_n\n",
    "    \\end{bmatrix} \\begin{bmatrix}q_1, q_2, \\cdots, q_n\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    p_1q_1 & p_1q_2 & \\dots  & p_1q_n \\\\\n",
    "    p_2q_1 & p_2q_2 & \\dots  & p_2q_n \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    p_nq_1 & p_nq_2 & \\dots  & p_nq_n \\\\\n",
    "\\end{bmatrix} \\tag{10}$$\n",
    "\n",
    "$$ \\begin{bmatrix}p_1q_1, p_2q_2, \\cdots, p_nq_n\\end{bmatrix} \\tag{11}$$\n",
    "\n",
    "\n",
    "下面分别采用常规方式和向量化方式实现dot/outer/elementwise product 操作，请注意两者的差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个向量 x1 和 x2\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "#----- x1 与 x2 的点积（或标量积/內积）实现方法\n",
    "# 可以看做 行向量x1 与 列向量 x2 的矩阵乘法 \n",
    "dot = 0 # 保存点积结果，是一个标量\n",
    "print('常规方法：')\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i] \n",
    "print('\\tdot = ', dot)\n",
    "\n",
    "print('向量化方法：')\n",
    "dot = np.dot(x1, x2)\n",
    "print('\\tdot = ', dot)\n",
    "\n",
    "#----- x1 与 x2 外积（或叉积/矢量积）的常规实现方法\n",
    "# 可以看做 列向量x1 与 行向量 x2 的矩阵乘法 \n",
    "outer = np.zeros((len(x1), len(x2)))  # x1 和 x2 外积的结果是一个  len(x1) × len(x2) 的矩阵\n",
    "print('常规方法：')\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i, j] = x1[i] * x2[j] \n",
    "print(\"\\touter = \", outer)\n",
    "\n",
    "print('向量化方法：')\n",
    "dot = np.outer(x1, x2)\n",
    "print('\\touter = ', outer)\n",
    "\n",
    "#----- x1 与 x2 逐元素相乘的常规实现方法\n",
    "# x1和x2的尺寸需一致 \n",
    "mul = np.zeros(len(x1)) # 结果是一个向量\n",
    "print('常规方法：')\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i] \n",
    "print(\"\\tmul = \",mul)\n",
    "print('向量化方法：')\n",
    "mul = np.multiply(x1, x2)\n",
    "print(\"\\tmul = \",mul)\n",
    "\n",
    "#--- 矩阵与向量点乘\n",
    "W = np.random.rand(3, len(x1))  # Random 3*len(x1) numpy array \n",
    "print('常规方法：')\n",
    "gdot = np.zeros(W.shape[0]) \n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i, j] * x1[j] \n",
    "print(\"\\tgdot = \" + str(gdot))\n",
    "\n",
    "print('向量化方法：')\n",
    "gdot = np.dot(W, x1) \n",
    "print(\"\\tgdot = \" + str(gdot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 此时的向量化是借助numpy库来实现，比起常规方式更为简洁和高效。\n",
    "* `np.dot()` 函数主要有两个功能：向量点积（输入为两个向量）和矩阵乘法（输入为两个矩阵，或者一个矩阵一个向量）。其与表示逐元素相乘的`np.multiply()`或者 `*` 操作符是有差别的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 计时\n",
    "Python自带time模块可以用于时间统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to show time by process_time() \n",
    "from time import process_time\n",
    "from time import process_time_ns\n",
    "  \n",
    "n = 500 \n",
    "   \n",
    "t1_start = process_time() \n",
    "t1_start_ns = process_time_ns() \n",
    "   \n",
    "# 需要统计时间的代码块\n",
    "for i in range(n): \n",
    "    for j in range(n):   \n",
    "        a = i*j\n",
    " \n",
    "t1_stop = process_time()\n",
    "t1_stop_ns = process_time_ns()\n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "print(\"Elapsed time in nanoseconds:\", t1_stop_ns, t1_start_ns)\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_stop-t1_start) \n",
    "print(\"Elapsed time during the whole program in nanoseconds:\", t1_stop_ns-t1_start_ns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 实现 L1 and L2 loss functions\n",
    "\n",
    "### 练习 8 - L1 \n",
    "- **定义**：\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{12}$$\n",
    "- **目标**：借助Numpy实现L1损失函数，用于表征预测值 ($ \\hat{y} $) 与真实值 ($y$)之间的差距. 在深度学习中，一般采用梯度下降法来优化该函数\n",
    "- 提示：\n",
    "    - 使用 `np.abs()` 可以求绝对值\n",
    "    - 使用 `np.sum()` 可以进行求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c496fe0b5fb6fe1580162305cf97387",
     "grade": false,
     "grade_id": "cell-410accbd4d9a1fc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "\"\"\"\n",
    "def L1(yhat, y):\n",
    "    # (≈ 1 line of code)\n",
    "    # loss =\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7435251c8f0959006b7034fd1eb9a2d3",
     "grade": true,
     "grade_id": "cell-44ac3b50c1fba7a0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "L1_test(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 9 - L2\n",
    "- **定义**：\n",
    "\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{13}$$\n",
    "\n",
    "- **目标**：借助Numpy实现L2损失函数。\n",
    "- 提示：\n",
    "如果 $x = [x_1, x_2, ..., x_n]$, 则 `np.dot(x,x)` = $\\sum_{j=1}^n x_j^{2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d806d7037061895561c70f6abb03380e",
     "grade": false,
     "grade_id": "cell-a2624d0db4d22322",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "\"\"\"\n",
    "def L2(yhat, y):\n",
    "    # (≈ 1 line of code)\n",
    "    # loss = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef616282fe941f332052dbb8641e9aa8",
     "grade": true,
     "grade_id": "cell-e7809ad65b5fe0ab",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat, y)))\n",
    "L2_test(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "<b>What to remember:</b>\n",
    "    \n",
    "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "- You have reviewed the L1 and L2 loss.\n",
    "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "351.198px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "eff7977957b28ff636dc016f58522771434b60a322db7b1d3763da27488278e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
