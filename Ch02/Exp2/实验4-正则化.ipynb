{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验4 正则化\n",
    "\n",
    "虽然深度学习模型具有强大的灵活性和性能，但如果训练数据集不足时，可能存在**过拟合**问题（这是一种很严重的问题）。即它在训练集上能表现出很好的性能，但在测试集上可能表现不佳，换句话说，模型的**泛化能力**不足 !\n",
    "\n",
    "目标：在你的深度模型中使用正规化（regularization）方法，防止过拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "from lib_reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n",
    "from lib_reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n",
    "from lib_testCases import *\n",
    "from lib_public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你刚刚被法国足球公司聘为人工智能专家。他们想让你给法国队守门员推荐踢球的落点位置，使得法国球员能够尽可能抢到第一落点。\n",
    "\n",
    "<img src=\"images/field_kiank.png\" style=\"width:600px;height:350px;\">\n",
    "<caption><center> <u> <b>Figure 1</b> </u>: <b>Football field</b><br> The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head </center></caption>\n",
    "\n",
    "他们给出了法国过去10场比赛的2D数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "法国门将从左向右发出球，每个点对应的是足球场上的一个位置：\n",
    "- 如果该点是蓝色，意味着法国球员能够得到球\n",
    "- 如果该点是红色，意味着对方球员能够得到球\n",
    "\n",
    "**你的目标**: 使用深度学习模型找出门将应该将球发至场上的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据集分析**: 该数据集虽然包含一定噪声, 但看起来可以用一条对角线将蓝色区域和红色区域进行分隔。\n",
    "\n",
    "你将首先尝试使用一个没有正规化的模型，然后你将学习如何施加正规化，并确定哪个模型更适合解决该问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - 无正规化模型\n",
    "\n",
    "你将使用下面的神经网络（相关代码已实现），其中：\n",
    "- 当`lambd`的值非0时，模型将进行*正规化*（注：由于\"`lambda`\"是python的保留关键字，因此这里使用\"`lambd`\"来替代）\n",
    "- 当`keep_prob`的值小于1时，模型将进行*dropout*\n",
    "\n",
    "你将首先尝试不适用正规化，然后你将实现：\n",
    "- *L2 正规化* -- 涉及函数: \"`compute_cost_with_regularization()`\" 和 \"`backward_propagation_with_regularization()`\"\n",
    "- *Dropout* -- 涉及函数: \"`forward_propagation_with_dropout()`\" 和 \"`backward_propagation_with_dropout()`\"\n",
    "\n",
    "在每个部分，你使用恰当的输入来运行模型，仔细阅读下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, print the cost every 10000 iterations\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n",
    "                                                # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认参数情况下将不进行正规化和Dropout操作，让我们训练模型并观察在数据集上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集和测试集上的准确率分别为94.8%和91.5%.\n",
    "这是我们的基准模型（**baseline model**），运行下列代码来绘制问题的决策边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model without regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型显然在训练集上产生了过拟合. 它拟合了一些干扰点! 让我们接下来看看2种降低过拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - L2 正规化\n",
    "\n",
    "避免过拟合的标准方法被称为 **L2 正规化**. 即将原始的代价函数:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "修改为:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "让我们修改你的代价函数并观察修改后的结果。\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### 练习 1 - compute_cost_with_regularization\n",
    "实现 `compute_cost_with_regularization()` 用来计算公式（2）中描述的代价函数。\n",
    "*提示*：可以使用下面的python代码来计算 $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$。\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e54417c158ef5260e3107ab846463e",
     "grade": false,
     "grade_id": "cell-02a896d283f479aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "\"\"\"\n",
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    #(≈ 1 lines of code)\n",
    "    # L2_regularization_cost = \n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8efc715a4d6127a214a1b9f97e9f4cb",
     "grade": true,
     "grade_id": "cell-8a99b24d8ecfe0c3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A3, t_Y, parameters = compute_cost_with_regularization_test_case()\n",
    "cost = compute_cost_with_regularization(A3, t_Y, parameters, lambd=0.1)\n",
    "print(\"cost = \" + str(cost))\n",
    "\n",
    "compute_cost_with_regularization_test(compute_cost_with_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于你修改了代价函数，也需要同步修改反向传播，即修改各个偏导计算方法。\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### 练习 2 - backward_propagation_with_regularization\n",
    "修改 dW1, dW2 以及 dW3的计算方法，添加正规项梯度 ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb2dfa385aa47fe2e2edf5c6821618e6",
     "grade": false,
     "grade_id": "cell-c6f6ed3630e04d4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "\"\"\"\n",
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    dZ3 = A3 - Y\n",
    "    #(≈ 1 lines of code)\n",
    "    # dW3 = 1./m * np.dot(dZ3, A2.T) + ？\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    #(≈ 1 lines of code)\n",
    "    # dW2 = 1./m * np.dot(dZ2, A1.T) + ?\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    #(≈ 1 lines of code)\n",
    "    # dW1 = 1./m * np.dot(dZ1, X.T) + ?\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd8e0024ad54c2facd2fb8e11d21d2a0",
     "grade": true,
     "grade_id": "cell-9826510f7bfdd0f8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_X, t_Y, cache = backward_propagation_with_regularization_test_case()\n",
    "\n",
    "grads = backward_propagation_with_regularization(t_X, t_Y, cache, lambd = 0.7)\n",
    "print (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\n",
    "print (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\n",
    "print (\"dW3 = \\n\"+ str(grads[\"dW3\"]))\n",
    "backward_propagation_with_regularization_test(backward_propagation_with_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行使用 L2 正规化的模型 $(\\lambda = 0.7)$. 函数 `model()` 将调用:\n",
    "- `compute_cost_with_regularization` 替代 `compute_cost`\n",
    "- `backward_propagation_with_regularization` 替代 `backward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, lambd = 0.7)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你, 测试集的准确率上升至93%. 让我们再次绘制决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with L2-regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**观察**:\n",
    "- $\\lambda$ 是一个超参数，可以根据验证集来进行调试\n",
    "- L2 正规化使得你的决策边界变得平滑。如果 $\\lambda$ 过大, 可能很过分平滑 \"oversmooth\", 使得模型具有较高的偏差(bias).\n",
    "\n",
    "**L2正规化做了什么？**:\n",
    "\n",
    "L2-正规化依据下面的假设：一个模型的权重越小则越简单. 因此，通过在代价函数中加入权重的平方作为惩罚，可以使得权重值变小，模型更平滑（当输入改变时，输出改变得更慢）\n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember:** the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost.\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices.\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Dropout\n",
    "\n",
    "最后，**dropout** 是一种深度学习中被广泛应用的正规化技术\n",
    "**它在每次迭代中随机失活一些神经元.**\n",
    "\n",
    "<!--\n",
    "To understand drop-out, consider this conversation with a friend:\n",
    "- Friend: \"Why do you need all these neurons to train your network and classify images?\". \n",
    "- You: \"Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!\"\n",
    "- Friend: \"I see, but are you sure that your neurons are learning different features and not all the same features?\"\n",
    "- You: \"Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution.\"\n",
    "!--> \n",
    "\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "<br>\n",
    "<caption><center> <u> <b>Figure 2 </b></u>: <b>Drop-out on the second hidden layer.</b> <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </center></caption>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> <u> <b>Figure 3</b> </u>:<b> Drop-out on the first and third hidden layers. </b><br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption>\n",
    "\n",
    "当你关闭一些神经元时，你实际上改变了模型。Dropout的核心思想是：在每次迭代中你训练一个不同的模型，它仅使用部分的神经元。\n",
    "通过dropout，你的神经元对于某些特定的神经元变得不那么敏感，因为其他神经元可能随时失活。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - 具有Dropout的前向传播\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "#### 练习 3 - forward_propagation_with_dropout\n",
    "\n",
    "在前向传播中实现dropout。在3层神经网络中，将在第1、2层上进行dropout，不改变输入输出层。\n",
    "\n",
    "**说明**:\n",
    "按照下面的4个步骤实现上述目标：\n",
    "1. 创建一个变量 $d^{[1]}$ 与$a^{[1]}$具有相同的形状尺寸，使用 `np.random.rand()` 来得到[0,1]随机数。这里你使用一个向量化的实现, 因此将创建一个随机矩阵 $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ 与 $A^{[1]}$ 具有相同的维度\n",
    "2. $D^{[1]}$ 中的元素按照一定的概率(`keep_prob`)取值1, 否则取0\n",
    "\n",
    "**提示:**\n",
    "当 keep_prob = 0.8, 意味着希望保留 80% 的神经元，其他20%将失活，那么$D^{[1]}$ 中有80%的元素为1，20%的元素为0\n",
    "Python代码\n",
    "`X = (X < keep_prob).astype(int)`\n",
    "与下面的if-else代码具有相同的效果(for the simple case of a one-dimensional array) :\n",
    "```\n",
    "for i,v in enumerate(x):\n",
    "    if v < keep_prob:\n",
    "        x[i] = 1\n",
    "    else: # v >= keep_prob\n",
    "        x[i] = 0\n",
    "```\n",
    "注意：\n",
    "- `X = (X < keep_prob).astype(int)` 可用于多维数组, 结果将保持与输入一直的维度。\n",
    "- 这里的`.astype(int)`起到强制类型转换的作用，即将原来的布尔值结果显式地转换为整形结果，尽管Python具有自动转换功能（如将布尔值乘以某数值，将自动把True转换为1，False转换为0）。\n",
    "\n",
    "\n",
    "3. 将 $A^{[1]}$ 修改为 $A^{[1]} * D^{[1]}$. 可以将 $D^{[1]}$ 想象为一个 mask, 即保留1对应的元素，丢地0对应的元素\n",
    "4. 将 $A^{[1]}$ 除以 `keep_prob`。该技术被称为inverted dropout。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "249ddfb0abac7c799948d3e600db7a4c",
     "grade": false,
     "grade_id": "cell-a81658747a0683be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "        W1 -- weight matrix of shape (20, 2)\n",
    "        b1 -- bias vector of shape (20, 1)\n",
    "        W2 -- weight matrix of shape (3, 20)\n",
    "        b2 -- bias vector of shape (3, 1)\n",
    "        W3 -- weight matrix of shape (1, 3)\n",
    "        b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "\"\"\"\n",
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    np.random.seed(1)\n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n",
    "    # D1 =                       # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    # D1 =                       # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    # A1 =                       # Step 3: shut down some neurons of A1\n",
    "    # A1 =                       # Step 4: scale the value of neurons that haven't been shut down\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    #(≈ 4 lines of code)\n",
    "    # D2 =                       # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    # D2 =                       # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    # A2 =                       # Step 3: shut down some neurons of A2\n",
    "    # A2 =                       # Step 4: scale the value of neurons that haven't been shut down\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "caec66931ac05dbe474596e75f3a14cd",
     "grade": true,
     "grade_id": "cell-be6195c629f586bf",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_X, parameters = forward_propagation_with_dropout_test_case()\n",
    "\n",
    "A3, cache = forward_propagation_with_dropout(t_X, parameters, keep_prob=0.7)\n",
    "print (\"A3 = \" + str(A3))\n",
    "\n",
    "forward_propagation_with_dropout_test(forward_propagation_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - 具有Dropout的反向传播\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "#### 练习 4 - backward_propagation_with_dropout\n",
    "在反向传播中实现dropout，将用到cache中保存的masks $D^{[1]}$ 和 $D^{[2]}$。\n",
    "\n",
    "**说明**:\n",
    "按照下面的2个步骤实现上述目标：\n",
    "1. 在前向传播中通过mask $D^{[1]}$ 作用于 `A1`实现失活，在反向传播中需要进行同样的操作。\n",
    "2. 在前向传播中将 `A1` 除以 `keep_prob`，在反向传播中需要进行同样的操作(解释：如果 $A^{[1]}$ 被放缩了`keep_prob`, 则它的倒数$dA^{[1]}$将被放缩相同的倍数)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee4145889a9c078fcf6aef51aceb3ba9",
     "grade": false,
     "grade_id": "cell-5b97731b540b0b87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "\"\"\"\n",
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    # dA2 =                # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    # dA1 =                # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c30bb7a9f59c7d421c8627d5d9252b29",
     "grade": true,
     "grade_id": "cell-958c189ce5b16569",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_X, t_Y, cache = backward_propagation_with_dropout_test_case()\n",
    "\n",
    "gradients = backward_propagation_with_dropout(t_X, t_Y, cache, keep_prob=0.8)\n",
    "\n",
    "print (\"dA1 = \\n\" + str(gradients[\"dA1\"]))\n",
    "print (\"dA2 = \\n\" + str(gradients[\"dA2\"]))\n",
    "\n",
    "backward_propagation_with_dropout_test(backward_propagation_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们运行具有 dropout 功能的模型 (`keep_prob = 0.86`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
    "\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见Dropout的功能很好，测试集的准确率进一步提升了 (达到95%)!\n",
    "运行下面的代码绘制决策边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**:\n",
    "- 一个常见的**错误方式**是在训练和测试时均使用dropout，正确的方式是仅在训练时进行dropout.\n",
    "- 深度学习框架例如 [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) 或者 [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) 均实现了 dropout 层.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember about dropout:**\n",
    "- Dropout is a regularization technique.\n",
    "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
    "- Apply dropout both during forward and backward propagation.\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**三个模型结果的统计表**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        <b>model</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>train accuracy</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>test accuracy</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN without regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "        <td>\n",
    "        91.5%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with L2-regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for finishing this assignment! And also for revolutionizing French football. :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What we want you to remember from this notebook**:\n",
    "- Regularization will help you reduce overfitting.\n",
    "- Regularization will drive your weights to lower values.\n",
    "- L2 regularization and Dropout are two very effective regularization techniques."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "SXQaI",
   "launcher_item_id": "UAwhh"
  },
  "kernelspec": {
   "name": "dl_course",
   "language": "python",
   "display_name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
