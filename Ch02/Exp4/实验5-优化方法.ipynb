{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验5 优化方法\n",
    "是否拥有一个良好的优化算法，可能是等待数天与仅仅几个小时就能获得好结果的区别。\n",
    "**任务**：\n",
    "* 应用随机梯度下降(Stochastic Gradient Descent)、动量（Momentum）、RMSProp、Adam等优化方法\n",
    "* 使用随机 minibatches 来加速收敛，改进优化\n",
    "\n",
    "依据代价函数$J$进行梯度下降可以理解为“下山”的过程：\n",
    "<img src=\"images/cost.jpg\" width=1000>\n",
    "<caption><center> <u> <b>Figure 1</b> </u>: <b>Minimizing the cost is like finding the lowest point in a hilly landscape</b><br> At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. </center></caption>\n",
    "\n",
    "**Notations**: As usual, $\\frac{\\partial J}{\\partial a } = $ `da` for any variable `a`.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from lib_opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from lib_opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from copy import deepcopy\n",
    "from lib_testCases import *\n",
    "from lib_public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - 梯度下降\n",
    "机器学习中的一种经典的优化方法是梯度下降（gradient descent，GD）。 如果你一次性对所有$m$个样本实施梯度下降，这称之为Batch Gradient Descent。\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### 练习 1 - update_parameters_with_gd\n",
    "实现梯度下降法则，即对于$l = 1, ..., L$:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
    "\n",
    "其中 *L* 表示层数， $\\alpha$ 是学习率.\n",
    "所有的参数应该存储在字典 `parameters` 中.\n",
    "注意，`for`循环中的计数变量 `l` 是从1开始的，因此第一个参数是$W^{[1]}$ 和 $b^{[1]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e464eca4306181b7b2d7908c2543cb4",
     "grade": false,
     "grade_id": "cell-7ed1efcf9ec96292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Update parameters using one step of gradient descent\n",
    "Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "        parameters['W' + str(l)] = Wl\n",
    "        parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "        grads['dW' + str(l)] = dWl\n",
    "        grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "\"\"\"\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    # Update rule for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        # (approx. 2 lines)\n",
    "        # parameters[\"W\" + str(l)] =  \n",
    "        # parameters[\"b\" + str(l)] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98daeabded9dffaa6da2915192be14c3",
     "grade": true,
     "grade_id": "cell-01dafb2c412914df",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "learning_rate = 0.01\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))\n",
    "\n",
    "update_parameters_with_gd_test(update_parameters_with_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD的一个变种是随机梯度下降(Stochastic Gradient Descent，SGD），当使用mini-batch且mini-batch size=1时，等价于SGD。\n",
    "在SGD中，上述参数更新法则并未改变。不同在于：一次只对一个训练样本进行梯度下降，而非整个训练样本。\n",
    "下面的代码展示了SGD和 (batch)GD之间的差别.\n",
    "\n",
    "- **(Batch) 梯度下降**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost.\n",
    "    cost += compute_cost(a, Y)\n",
    "    # Backward propagation.\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **随机梯度下降**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost += compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当训练样本非常大时，SGD要更快一些，但参数会**震荡**地趋向于最小值，而非平滑地收敛。\n",
    "\n",
    "<img src=\"images/kiank_sgd.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : <b>SGD vs GD</b><br> \"+\" denotes a minimum of the cost. SGD leads to many oscillations to reach convergence, but each step is a lot faster to compute for SGD than it is for GD, as it uses only one training example (vs. the whole batch for GD). </center></caption>\n",
    "\n",
    "**注意** 上面实现 SGD 的代码展示其需要使用3个循环\n",
    "\n",
    "实际应用中，常常可以通过Mini-batch梯度下降来获取较快的速度，其既不使用所有训练样本，也不仅使用1个训练样本，而是对特定数量的训练样本实时梯度下降。\n",
    "\n",
    "<img src=\"images/kiank_minibatch.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 2</b> </u>: <font color='purple'>  <b>SGD vs Mini-Batch GD</b><br> \"+\" denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Mini-Batch 梯度下降\n",
    "从训练集$(X, Y)$构建mini-batches数据\n",
    "\n",
    "具体来说需两步:\n",
    "1. **打乱（Shuffle）**: 构建一个乱序版本的训练集$(X, Y)$，如下所示。$X$和$Y$的每列表示一个训练样本.\n",
    "注意，随机乱序对于*X*和*Y*来说需要同步进行，即打乱后*X*的第 $i$ 列与*Y*的第 $i$ 列应该依然对应同一个样本。 乱序操作保证样本可以被随机分配到不同的mini-batches中。\n",
    "\n",
    "<img src=\"images/kiank_shuffle.png\" width=1000>\n",
    "\n",
    "2. **切分**: 将乱序后的$(X, Y)$切分至大小为`mini_batch_size` (这里取值 64)的mini-batches中. 注意，训练样本的个数不一定能够被`mini_batch_size`整除. 最后一个mini batch的个数可能会小于`mini_batch_size`, 如下图所示:\n",
    "\n",
    "<img src=\"images/kiank_partition.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='ex-2'></a>\n",
    "### 练习 2 - random_mini_batches\n",
    "实现 `random_mini_batches`.\n",
    "*提示*：\n",
    "- 乱序步骤：相关代码已给出，请仔细阅读\n",
    "- 切分步骤：下面展示了分配第1个和第2个mini-batches的代码:\n",
    "```python\n",
    "first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\n",
    "second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n",
    "...\n",
    "```\n",
    "注意：对于最后一个mini-batch样本个数可能小于`mini_batch_size=64`的问题. 令 $\\lfloor s \\rfloor$ 表示对 $s$ 进行向下取整 (`math.floor(s)`). 如果训练样本总数不是`mini_batch_size=64`的倍数，则将有$\\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor$大小为64的 mini-batches, 以及最后一个大小为$\\left(m-mini_\\_batch_\\_size \\times \\left\\lfloor \\frac{m}{mini\\_batch\\_size}\\right\\rfloor\\right)$的mini-batch。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e276742d3477f18007b3d340b0039271",
     "grade": false,
     "grade_id": "cell-a693afffedab4203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "\"\"\"\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    inc = mini_batch_size\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        # (approx. 2 lines)\n",
    "        # mini_batch_X =  \n",
    "        # mini_batch_Y =\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        #(approx. 2 lines)\n",
    "        # mini_batch_X =\n",
    "        # mini_batch_Y =\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9a50964c5ab5622435c64a4f7d9e44a",
     "grade": true,
     "grade_id": "cell-9bd796497095573b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t_X, t_Y, mini_batch_size = random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(t_X, t_Y, mini_batch_size)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n",
    "\n",
    "random_mini_batches_test(random_mini_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "- Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - 动量\n",
    "使用动量可以缓解mini-batch梯度下降中的“震荡”问题.\n",
    "前一次梯度的“方向”将被存储在变量 $v$ 中.\n",
    "\n",
    "Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.\n",
    "\n",
    "<img src=\"images/opt_momentum.png\" style=\"width:400px;height:250px;\">\n",
    "\n",
    "The red arrows show the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, the gradient is allowed to influence $v$ and then take a step in the direction of $v$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>    \n",
    "### 练习 3 - initialize_velocity\n",
    "初始化速度$v$, 它是一个python字典，需要使用全零的数组来初始化.\n",
    "其“键”与`grads`字典中的一致，包括:\n",
    "```python\n",
    "v[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\n",
    "v[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n",
    "```\n",
    "$l =1,...,L$\n",
    "\n",
    "**注意** *l*从1开始计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73f98ff4232b1eb6ddd045f4a052d495",
     "grade": false,
     "grade_id": "cell-667cf6695880506a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "        - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "        - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "        parameters['W' + str(l)] = Wl\n",
    "        parameters['b' + str(l)] = bl\n",
    "Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "        v['dW' + str(l)] = velocity of dWl\n",
    "        v['db' + str(l)] = velocity of dbl\n",
    "\"\"\"\n",
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    # Initialize velocity\n",
    "    for l in range(1, L + 1):\n",
    "        # (approx. 2 lines)\n",
    "        # v[\"dW\" + str(l)] =\n",
    "        # v[\"db\" + str(l)] =\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09a811f004e96833a7e6cc47a55de653",
     "grade": true,
     "grade_id": "cell-c129a0130218c80f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = initialize_velocity_test_case()\n",
    "\n",
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))\n",
    "\n",
    "initialize_velocity_test(initialize_velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>   \n",
    "### 练习 4 - update_parameters_with_momentum\n",
    "接下来，实现使用动量来更新参数。更新法则为, 对于 $l = 1, ..., L$:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$\n",
    "\n",
    "其中 *L* 是层数, $\\beta$ 是动量值， $\\alpha$ 是学习率。所有的参数都保存在字典 `parameters` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09ff6600367fba5cb96155b80a2b3688",
     "grade": false,
     "grade_id": "cell-a5f80aecc1d4e020",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Update parameters using Momentum\n",
    "Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "        parameters['W' + str(l)] = Wl\n",
    "        parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "        grads['dW' + str(l)] = dWl\n",
    "        grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "        v['dW' + str(l)] = ...\n",
    "        v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    v -- python dictionary containing your updated velocities\n",
    "\"\"\"\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    # Momentum update for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        # (approx. 4 lines)\n",
    "        # compute velocities\n",
    "        # v[\"dW\" + str(l)] = ...\n",
    "        # v[\"db\" + str(l)] = ...\n",
    "        # update parameters\n",
    "        # parameters[\"W\" + str(l)] = ...\n",
    "        # parameters[\"b\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01b9bf272f5f4d7ed4e26ca3fb956b9b",
     "grade": true,
     "grade_id": "cell-4c7cb001c56beb5d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
    "\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))\n",
    "\n",
    "update_parameters_with_momentum_test(update_parameters_with_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that**:\n",
    "- The velocity is initialized with zeros. So the algorithm will take a few iterations to \"build up\" velocity and start to take bigger steps.\n",
    "- If $\\beta = 0$, then this just becomes standard gradient descent without momentum. \n",
    "\n",
    "**How do you choose $\\beta$?**\n",
    "\n",
    "- The larger the momentum $\\beta$ is, the smoother the update, because it takes the past gradients into account more. But if $\\beta$ is too big, it could also smooth out the updates too much. \n",
    "- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n",
    "- Tuning the optimal $\\beta$ for your model might require trying several values to see what works best in terms of reducing the value of the cost function $J$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n",
    "- You have to tune a momentum hyperparameter $\\beta$ and a learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>   \n",
    "## 5 - Adam\n",
    "\n",
    "**Adam的工作原理**\n",
    "Adam中参数的更新法则为, 对于 $l = 1, ..., L$:\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "其中:\n",
    "- t Adam步骤的计数\n",
    "- L 是层数\n",
    "- $\\beta_1$ 和 $\\beta_2$ 是控制两种指数加权平均的超参数\n",
    "- $\\alpha$ 是学习率\n",
    "- $\\varepsilon$ 是一个非常小的数，避免分母为0\n",
    "\n",
    "与之前相同，所有的参数都保存在字典`parameters`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-5'></a>   \n",
    "### 练习 5 - initialize_adam\n",
    "初始化Adam变量 $v, s$，用来追踪之前的信息\n",
    "\n",
    "**指南**: 变量 $v, s$ 是 python 字典，需要被初始化为全零的数组，其“键”与 `grads` 中一致, 即:\n",
    "```python\n",
    "v[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\n",
    "v[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n",
    "s[\"dW\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l)])\n",
    "s[\"db\" + str(l)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l)])\n",
    "```\n",
    "\n",
    "$l = 1, ..., L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8eb19ce4b30a9c2af428853c24d8b80a",
     "grade": false,
     "grade_id": "cell-f985b4ecf2e3b4b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "        - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "        - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "        parameters[\"W\" + str(l)] = Wl\n",
    "        parameters[\"b\" + str(l)] = bl\n",
    "Returns:\n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
    "        v[\"dW\" + str(l)] = ...\n",
    "        v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
    "        s[\"dW\" + str(l)] = ...\n",
    "        s[\"db\" + str(l)] = ...\n",
    "\"\"\"\n",
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "        # (approx. 4 lines)\n",
    "        # v[\"dW\" + str(l)] = ...\n",
    "        # v[\"db\" + str(l)] = ...\n",
    "        # s[\"dW\" + str(l)] = ...\n",
    "        # s[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63c23c13e1cfb6e1c04b62541ea07cae",
     "grade": true,
     "grade_id": "cell-66f5f68aa23508d7",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = initialize_adam_test_case()\n",
    "\n",
    "v, s = initialize_adam(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))\n",
    "\n",
    "initialize_adam_test(initialize_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-6'></a>   \n",
    "### 练习 6 - update_parameters_with_adam\n",
    "接下来使用Adam方法更新参数. 对于 $l = 1, ..., L$:\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "**Note** that the iterator `l` starts at 1 in the `for` loop as the first parameters are $W^{[1]}$ and $b^{[1]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db9c45f6bfd6b1395c2662f6f127d12b",
     "grade": false,
     "grade_id": "cell-d72b0d5fd3ac5c42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function:\n",
    "    Update parameters using Adam\n",
    "Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "        parameters['W' + str(l)] = Wl\n",
    "        parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "        grads['dW' + str(l)] = dWl\n",
    "        grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- Adam variable, counts the number of taken steps\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "\"\"\"\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        # (approx. 2 lines)\n",
    "        # v[\"dW\" + str(l)] = ...\n",
    "        # v[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        # (approx. 2 lines)\n",
    "        # v_corrected[\"dW\" + str(l)] = ...\n",
    "        # v_corrected[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        #(approx. 2 lines)\n",
    "        # s[\"dW\" + str(l)] = ...\n",
    "        # s[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        # (approx. 2 lines)\n",
    "        # s_corrected[\"dW\" + str(l)] = ...\n",
    "        # s_corrected[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        # (approx. 2 lines)\n",
    "        # parameters[\"W\" + str(l)] = ...\n",
    "        # parameters[\"b\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3fe7eb303d8942f4e51a1f6afe587bb",
     "grade": true,
     "grade_id": "cell-c2a35a4cdbfa242c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parametersi, grads, vi, si = update_parameters_with_adam_test_case()\n",
    "\n",
    "t = 2\n",
    "learning_rate = 0.02\n",
    "beta1 = 0.8\n",
    "beta2 = 0.888\n",
    "epsilon = 1e-2\n",
    "\n",
    "parameters, v, s, vc, sc  = update_parameters_with_adam(parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon)\n",
    "print(f\"W1 = \\n{parameters['W1']}\")\n",
    "print(f\"W2 = \\n{parameters['W2']}\")\n",
    "print(f\"b1 = \\n{parameters['b1']}\")\n",
    "print(f\"b2 = \\n{parameters['b2']}\")\n",
    "\n",
    "update_parameters_with_adam_test(update_parameters_with_adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected values:**\n",
    "    \n",
    "```\n",
    "W1 = \n",
    "[[ 1.63942428 -0.6268425  -0.54320974]\n",
    " [-1.08782943  0.85036983 -2.2865723 ]]\n",
    "W2 = \n",
    "[[ 0.33356139 -0.26425199  1.47707772]\n",
    " [-2.04538458 -0.30744933 -0.36903141]\n",
    " [ 1.14873036 -1.09256871 -0.15734651]]\n",
    "b1 = \n",
    "[[ 1.75854357]\n",
    " [-0.74616067]]\n",
    "b2 = \n",
    "[[-0.89228024]\n",
    " [ 0.02707193]\n",
    " [ 0.56782561]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let's implement a model with each of these optimizers and observe the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>  \n",
    "## 6 - 在模型中验证上述优化方法\n",
    "下面你将使用名为 \"moons\" 的数据库来测试不同的优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3-layer neural network has already been implemented for you! You'll train it with: \n",
    "- Mini-batch **Gradient Descent**: it will call your function:\n",
    "    - `update_parameters_with_gd()`\n",
    "- Mini-batch **Momentum**: it will call your functions:\n",
    "    - `initialize_velocity()` and `update_parameters_with_momentum()`\n",
    "- Mini-batch **Adam**: it will call your functions:\n",
    "    - `initialize_adam()` and `update_parameters_with_adam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this 3 layer neural network with each of the 3 optimization methods.\n",
    "\n",
    "<a name='6-1'></a>  \n",
    "### 6.1 - Mini-Batch Gradient Descent\n",
    "\n",
    "Run the following code to see how the model does with mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>  \n",
    "### 6.2 - Mini-Batch Gradient Descent with Momentum\n",
    "\n",
    "Next, run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small - but for more complex problems you might see bigger gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Momentum optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>  \n",
    "### 6.3 - Mini-Batch with Adam\n",
    "\n",
    "Finally, run the following code to see how the model does with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6-4'></a>  \n",
    "### 6.4 - Summary\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        <b>optimization method</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>accuracy</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>cost shape</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        Gradient descent\n",
    "        </td>\n",
    "        <td>\n",
    "        >71%\n",
    "        </td>\n",
    "        <td>\n",
    "        smooth\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Momentum\n",
    "        </td>\n",
    "        <td>\n",
    "        >71%\n",
    "        </td>\n",
    "        <td>\n",
    "        smooth\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Adam\n",
    "        </td>\n",
    "        <td>\n",
    "        >94%\n",
    "        </td>\n",
    "        <td>\n",
    "        smoother\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible.\n",
    "\n",
    "On the other hand, Adam clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.\n",
    "\n",
    "Some advantages of Adam include:\n",
    "\n",
    "- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n",
    "- Usually works well even with little tuning of hyperparameters (except $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "- Adam paper: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>  \n",
    "## 7 - 学习率衰减和调度\n",
    "最后，学习率是另外一个可以加速学习过程的超参数\n",
    "\n",
    "训练一开始，你的模型可以大步走\n",
    "\n",
    "学习率衰减可以通过（1）自适应方法或者（2）预定义设置来实现\n",
    "\n",
    "接下来你将在一个三层的NN中使用预定义学习率衰减策略，并分别采样三种优化模型，观察他们的差别，以及不同学习率的效果\n",
    "\n",
    "该模型本质上跟之前使用的一样，差别在于使用了学习率衰减策略，包含2个参数 decay 和 decay_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate   # the original learning rate\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-1'></a>  \n",
    "### 7.1 - 在每轮迭代中衰减学习率\n",
    "你将尝试一种预定义的学习率衰减策略，称为指数学习率衰减. 其数学定义如下:\n",
    "\n",
    "$$\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}$$\n",
    "\n",
    "<a name='ex-7'></a>  \n",
    "#### 练习 7 - update_lr\n",
    "\n",
    "使用指数权重衰减方法计算新的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68d0f6e5b2a1a462ee981bf6c4ac6414",
     "grade": false,
     "grade_id": "cell-1f75dd71cfae785a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer\n",
    "    decay_rate -- Decay rate. Scalar\n",
    "Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar\n",
    "\"\"\"\n",
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    #(approx. 1 line)\n",
    "    # learning_rate = \n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e94cc1e45ead743ed2c013bea09a2170",
     "grade": true,
     "grade_id": "cell-84c8bdb20bc64216",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "epoch_num = 2\n",
    "decay_rate = 1\n",
    "learning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n",
    "\n",
    "print(\"Updated learning rate: \", learning_rate_2)\n",
    "\n",
    "update_lr_test(update_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=update_lr)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if you set the decay to occur at every iteration, the learning rate goes to zero too quickly - even if you start with a higher learning rate. \n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        <b>Epoch Number</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>Learning Rate</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>Cost</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        0\n",
    "        </td>\n",
    "        <td>\n",
    "        0.100000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.701091\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        1000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.000100\n",
    "        </td>\n",
    "        <td>\n",
    "        0.661884\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        2000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.000050\n",
    "        </td>\n",
    "        <td>\n",
    "        0.658620\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.000033\n",
    "        </td>\n",
    "        <td>\n",
    "        0.656765\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        4000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.000025\n",
    "        </td>\n",
    "        <td>\n",
    "        0.655486\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        5000\n",
    "        </td>\n",
    "        <td>\n",
    "        0.000020\n",
    "        </td>\n",
    "        <td>\n",
    "        0.654514\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "When you're training for a few epoch this doesn't cause a lot of troubles, but when the number of epochs is large the optimization algorithm will stop updating. One common fix to this issue is to decay the learning rate every few steps. This is called fixed interval scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-2'></a> \n",
    "### 7.2 - Fixed Interval Scheduling\n",
    "You can help prevent the learning rate speeding to zero too quickly by scheduling the exponential learning rate decay at a fixed time interval, for example 1000. You can either number the intervals, or divide the epoch by the time interval, which is the size of window with the constant learning rate.\n",
    "\n",
    "<img src=\"images/lr.png\" style=\"width:400px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-8'></a> \n",
    "#### 练习 8 - schedule_lr_decay\n",
    "\n",
    "Calculate the new learning rate using exponential weight decay with fixed interval scheduling.\n",
    "\n",
    "**Instructions**: Implement the learning rate scheduling such that it only changes when the epochNum is a multiple of the timeInterval.\n",
    "\n",
    "**Note:** The fraction in the denominator uses the floor operation. \n",
    "\n",
    "$$\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}$$\n",
    "\n",
    "**Hint:** [numpy.floor](https://numpy.org/doc/stable/reference/generated/numpy.floor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6684151ebcddc6e4aaad1040b9e3d80a",
     "grade": false,
     "grade_id": "cell-e5b733253d9006fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function：\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer.\n",
    "    decay_rate -- Decay rate. Scalar.\n",
    "    time_interval -- Number of epochs where you update the learning rate.\n",
    "Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar\n",
    "\"\"\"\n",
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    # (approx. 1 lines)\n",
    "    # learning_rate = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "221cccee108f8b2db6ff3c6c76ee3db9",
     "grade": true,
     "grade_id": "cell-03cd771ef9f3be85",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "\n",
    "epoch_num_1 = 10\n",
    "epoch_num_2 = 100\n",
    "decay_rate = 0.3\n",
    "time_interval = 100\n",
    "learning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\n",
    "learning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)\n",
    "\n",
    "schedule_lr_decay_test(schedule_lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Original learning rate:  0.5\n",
    "Updated learning rate after 10 epochs:  0.5\n",
    "Updated learning rate after 100 epochs:  0.3846153846153846\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3'></a> \n",
    "### 7.3 - Using Learning Rate Decay for each Optimization Method\n",
    "\n",
    "Below, you'll use the following \"moons\" dataset to test the different optimization methods. (The dataset is named \"moons\" because the data from each of the two classes looks a bit like a crescent-shaped moon.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3-1'></a> \n",
    "#### 7.3.1 - Gradient Descent with Learning Rate Decay\n",
    "\n",
    "Run the following code to see how the model does gradient descent and weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3-2'></a> \n",
    "#### 7.3.2 - Gradient Descent with Momentum and Learning Rate Decay\n",
    "\n",
    "Run the following code to see how the model does gradient descent with momentum and weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent with momentum optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-3-3'></a> \n",
    "#### 7.3.3 - Adam with Learning Rate Decay\n",
    "\n",
    "Run the following code to see how the model does Adam and weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\", learning_rate = 0.01, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7-4'></a> \n",
    "### 7.4 - Achieving similar performance with different methods\n",
    "\n",
    "With SGD or SGD with Momentum, the accuracy is significantly lower than Adam, but when learning rate decay is added on top, either can achieve performance at a speed and accuracy score that's similar to Adam.\n",
    "\n",
    "In the case of Adam, notice that the learning curve achieves a similar accuracy but faster.\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        <b>optimization method</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        <b>accuracy</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        Gradient descent\n",
    "        </td>\n",
    "        <td>\n",
    "        >94.6%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Momentum\n",
    "        </td>\n",
    "        <td>\n",
    "        >95.6%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Adam\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You've made it to the end of the Optimization methods notebook. Here's a quick recap of everything you're now able to do: \n",
    "\n",
    "* Apply three different optimization methods to your models \n",
    "* Build mini-batches for your training set \n",
    "* Use learning rate decay scheduling to speed up your training\n",
    "\n",
    "Great work!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "name": "dl_course",
   "language": "python",
   "display_name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
